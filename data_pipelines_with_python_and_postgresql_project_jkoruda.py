# -*- coding: utf-8 -*-
"""Data_Pipelines_with_Python_and_PostgreSQL_Project_JKoruda.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ye6Lg__LKblrLgH4YhD9EtPjXTqNx8TP
"""

# -*- coding: utf-8 -*-

!pip install psycopg2-binary
#import pandas
import pandas as pd
from sqlalchemy import create_engine

# Google cloud postre  connection
POSTGRES_ADDRESS = '157.245.102.81'
POSTGRES_PORT = '5432'
POSTGRES_DBNAME = 'dq'
POSTGRES_USERNAME = 'postgres'
POSTGRES_PASSWORD = 'E*3b8km$dpmRLLuf1Rs$'




# Define the database engine
postgres_str = ('postgresql://{username}:{password}@{ipaddress}:{port}/{dbname}'
                .format(username=POSTGRES_USERNAME,
                        password=POSTGRES_PASSWORD,
                        ipaddress=POSTGRES_ADDRESS,
                        port=POSTGRES_PORT,
                        dbname=POSTGRES_DBNAME))
engine = create_engine(postgres_str)

#Data Extraction

def extract_data():
    # Load the equipment sensor data
    equipment_data = pd.read_csv('equipment_sensor.csv')

    # Load the network sensor data
    network_data = pd.read_csv('network_sensor.csv')

    # Load the maintenance records data
    maintenance_data = pd.read_csv('maintenance_records.csv')

    return equipment_data, network_data, maintenance_data

#Transform Data

def transform_data(equipment_data, network_data, maintenance_data):
    # Remove duplicates
    equipment_data.drop_duplicates(inplace=True)
    network_data.drop_duplicates(inplace=True)
    maintenance_data.drop_duplicates(inplace=True)

    # Correct Missing data
    equipment_data.fillna(method='ffill', inplace=True)
    network_data.fillna(method='ffill', inplace=True)
    maintenance_data.fillna(method='ffill', inplace=True)

    # Achieve data consistency
    equipment_data['date_time'] = pd.to_datetime(equipment_data['date'] + ' ' + equipment_data['time'])
    equipment_data.drop(['date', 'time'], axis=1, inplace=True)

    network_data['date_time'] = pd.to_datetime(network_data['date'] + ' ' + network_data['time'])
    network_data.drop(['date', 'time'], axis=1, inplace=True)

    maintenance_data['date_time'] = pd.to_datetime(maintenance_data['date'] + ' ' + maintenance_data['time'])
    maintenance_data.drop(['date', 'time'], axis=1, inplace=True)

    # Aggregate the data
    equipment_summary = equipment_data.groupby('ID').agg({'date_time': ['min', 'max'], 'sensor_reading': ['mean', 'max']})
    equipment_summary.columns = ['first_seen', 'last_seen', 'average_reading', 'max_reading']
    network_summary = network_data.groupby('ID').agg({'date_time': ['min', 'max'], 'sensor_reading': ['mean', 'max']})
    network_summary.columns = ['first_seen', 'last_seen', 'average_reading', 'max_reading']

    # Join the data
    sensor_summary = pd.merge(equipment_summary, network_summary, how='outer', left_index=True, right_index=True)
    sensor_summary = sensor_summary.reset_index()
    sensor_summary = sensor_summary.rename(columns={'ID': 'equipment_ID'})

    maintenance_data = maintenance_data[['date_time', 'equipment_ID', 'maintenance_type']]

    return sensor_summary, maintenance_data

#Load data and define main function

def load_data(sensor_summary, maintenance_df):
    # Load the data to PostgreSQL
    sensor_summary.to_sql('sensor_summary', engine, if_exists='replace')
    maintenance_df.to_sql('maintenance_records', engine, if_exists='replace')

def main():
    equipment_data, network_data, maintenance_data = extract_data()
    sensor_summary, maintenance_df = transform_data(equipment_data, network_data, maintenance_data)
    load_data(sensor_summary, maintenance_df)

if __name__ == '__main__':
    main()